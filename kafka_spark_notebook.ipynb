{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76ff83f",
   "metadata": {},
   "source": [
    "# Notebook: Spark + Kafka + Wykresy\n",
    "Ten notebook pokazuje, jak wczytac dane z Kafki przez Spark, zapisac je do JSON, a potem zrobic agregacje i zestaw wykresow biznesowych.\n",
    "\n",
    "Wymagania:\n",
    "- pyspark\n",
    "- matplotlib\n",
    "- numpy\n",
    "- seaborn\n",
    "\n",
    "Jesli nie masz pakietow, zainstaluj je w tym samym srodowisku co notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4bd438",
   "metadata": {},
   "source": [
    "## 1. Skonfiguruj sesje Spark w notebooku\n",
    "Utworz SparkSession, ustaw konfiguracje i sprawdz, czy dziala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_version = pyspark.__version__\n",
    "if spark_version.startswith(\"4.\"):\n",
    "    scala_suffix = \"2.13\"\n",
    "elif spark_version.startswith(\"3.\"):\n",
    "    scala_suffix = \"2.12\"\n",
    "else:\n",
    "    scala_suffix = \"2.12\"\n",
    "\n",
    "kafka_package = f\"org.apache.spark:spark-sql-kafka-0-10_{scala_suffix}:{spark_version}\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"KafkaCrashCourse\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars.packages\", kafka_package)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark, kafka_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b709c",
   "metadata": {},
   "source": [
    "## 2. Zaladuj dane i wykonaj podstawowe czyszczenie\n",
    "Ponizej masz dwa warianty: streaming z Kafki do JSON oraz batch z plikow JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, lit, when\n",
    "from pyspark.sql.types import BooleanType, DoubleType, IntegerType, LongType, StringType, StructField, StructType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"item\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"discount_pct\", IntegerType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"sales_channel\", StringType(), True),\n",
    "    StructField(\"store_city\", StringType(), True),\n",
    "    StructField(\"purchase_datetime\", StringType(), True),\n",
    "    StructField(\"purchase_date\", StringType(), True),\n",
    "    StructField(\"purchase_time\", StringType(), True),\n",
    "    StructField(\"weekday_name\", StringType(), True),\n",
    "    StructField(\"weekday_num\", IntegerType(), True),\n",
    "    StructField(\"hour_of_day\", IntegerType(), True),\n",
    "    StructField(\"is_weekend\", BooleanType(), True),\n",
    "    StructField(\"event_time_ms\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Streaming: Kafka -> JSON (append)\n",
    "kafka_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092,localhost:9094\")\n",
    "    .option(\"subscribe\", \"orders\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "parsed_df = (\n",
    "    kafka_df.selectExpr(\"CAST(value AS STRING) AS json_str\")\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"data\"), col(\"json_str\"))\n",
    "    .select(\"data.*\", \"json_str\")\n",
    ")\n",
    "\n",
    "validated_df = parsed_df.withColumn(\n",
    "    \"invalid_reason\",\n",
    "    when(col(\"quantity\").isNull(), lit(\"missing_quantity\"))\n",
    "    .when(col(\"unit_price\").isNull(), lit(\"missing_unit_price\"))\n",
    "    .when(col(\"quantity\") <= 0, lit(\"non_positive_quantity\"))\n",
    "    .when(col(\"unit_price\") <= 0, lit(\"non_positive_unit_price\"))\n",
    "    .otherwise(lit(None).cast(StringType()))\n",
    ")\n",
    "\n",
    "valid_df = validated_df.where(col(\"invalid_reason\").isNull()).drop(\"invalid_reason\", \"json_str\")\n",
    "invalid_df = validated_df.where(col(\"invalid_reason\").isNotNull())\n",
    "\n",
    "output_path = \"data/orders_json\"\n",
    "checkpoint_path = \"data/_checkpoints/orders_json\"\n",
    "invalid_output_path = \"data/invalid_events\"\n",
    "invalid_checkpoint_path = \"data/_checkpoints/invalid_events\"\n",
    "\n",
    "valid_stream_query = (\n",
    "    valid_df.writeStream\n",
    "    .format(\"json\")\n",
    "    .option(\"path\", output_path)\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "invalid_stream_query = (\n",
    "    invalid_df.writeStream\n",
    "    .format(\"json\")\n",
    "    .option(\"path\", invalid_output_path)\n",
    "    .option(\"checkpointLocation\", invalid_checkpoint_path)\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# valid_stream_query.awaitTermination()\n",
    "(valid_stream_query, invalid_stream_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e2613f",
   "metadata": {},
   "source": [
    "## 3. Batch: odczyt JSON i czyszczenie\n",
    "Teraz wykonujemy odczyt zapisanych plikow JSON i czyszczenie danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b846455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch: odczyt JSON i czyszczenie\n",
    "# Uzywamy schematu, zeby kolumny byly znane nawet przy pustym katalogu.\n",
    "batch_df = spark.read.schema(schema).json(output_path)\n",
    "\n",
    "clean_df = (\n",
    "    batch_df\n",
    "    .select(\n",
    "        \"order_id\", \"user\", \"item\", \"category\", \"quantity\", \"unit_price\", \"discount_pct\", \"total_amount\",\n",
    "        \"payment_method\", \"sales_channel\", \"store_city\", \"purchase_datetime\", \"purchase_date\", \"purchase_time\",\n",
    "        \"weekday_name\", \"weekday_num\", \"hour_of_day\", \"is_weekend\", \"event_time_ms\"\n",
    "    )\n",
    "    .where(\n",
    "        col(\"item\").isNotNull()\n",
    "        & col(\"quantity\").isNotNull()\n",
    "        & (col(\"quantity\") > 0)\n",
    "        & col(\"unit_price\").isNotNull()\n",
    "        & (col(\"unit_price\") > 0)\n",
    "        & col(\"weekday_name\").isNotNull()\n",
    "        & col(\"weekday_num\").isNotNull()\n",
    "        & col(\"total_amount\").isNotNull()\n",
    "    )\n",
    ")\n",
    "\n",
    "clean_df.printSchema()\n",
    "print(f\"Rows: {clean_df.count()}\")\n",
    "clean_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-events-md",
   "metadata": {},
   "source": [
    "## 4. Nieprawidlowe eventy (odrzucone)\n",
    "Odczytujemy eventy odrzucone z dalszej analizy i zapisane do `data/invalid_events`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-events-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "invalid_schema = StructType(schema.fields + [\n",
    "    StructField(\"json_str\", StringType(), True),\n",
    "    StructField(\"invalid_reason\", StringType(), True),\n",
    "])\n",
    "\n",
    "if os.path.exists(invalid_output_path):\n",
    "    invalid_batch_df = spark.read.schema(invalid_schema).json(invalid_output_path)\n",
    "else:\n",
    "    invalid_batch_df = spark.createDataFrame([], invalid_schema)\n",
    "\n",
    "invalid_events_df = (\n",
    "    invalid_batch_df\n",
    "    .select(\"invalid_reason\", \"order_id\", \"item\", \"quantity\", \"unit_price\", \"json_str\", \"event_time_ms\")\n",
    "    .where(col(\"invalid_reason\").isNotNull())\n",
    ")\n",
    "\n",
    "print(f\"Invalid rows: {invalid_events_df.count()}\")\n",
    "invalid_events_df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092bb21e",
   "metadata": {},
   "source": [
    "## 5. Agreguj dane w Spark i przygotuj ramke do wykresu\n",
    "Agregujemy dane po produkcie i dniu tygodnia. Wykres 3D pokaze osie: produkt, dzien tygodnia i laczna ilosc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca978c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as sum_agg\n",
    "\n",
    "agg_df = (\n",
    "    clean_df\n",
    "    .groupBy(\"weekday_num\", \"weekday_name\", \"item\")\n",
    "    .agg(\n",
    "        sum_agg(\"quantity\").alias(\"total_quantity\"),\n",
    "        sum_agg(\"total_amount\").alias(\"total_revenue\")\n",
    "    )\n",
    "    .orderBy(col(\"weekday_num\"), col(\"total_quantity\").desc())\n",
    ")\n",
    "\n",
    "plot_df = agg_df.toPandas()\n",
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Szybkie podsumowanie lacznej ilosci i przychodu\n",
    "total_quantity_all = float(plot_df[\"total_quantity\"].sum())\n",
    "total_revenue_all = float(plot_df[\"total_revenue\"].sum())\n",
    "total_quantity_all, total_revenue_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12a2f4",
   "metadata": {},
   "source": [
    "## 6. Wykres 3D: produkt x dzien tygodnia x ilosc\n",
    "Kolor slupka reprezentuje laczny przychod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b10049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm, colors\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "if plot_df.empty:\n",
    "    print(\"Brak danych do wykresu. Najpierw uruchom streaming i wyslij eventy.\")\n",
    "else:\n",
    "    weekday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    item_order = sorted(plot_df[\"item\"].unique())\n",
    "    x_spacing = 1.55\n",
    "    item_to_x = {item: idx * x_spacing for idx, item in enumerate(item_order)}\n",
    "\n",
    "    x = plot_df[\"item\"].map(item_to_x).to_numpy(dtype=float)\n",
    "    y = plot_df[\"weekday_num\"].to_numpy(dtype=float)\n",
    "    z = np.zeros(len(plot_df), dtype=float)\n",
    "    dx = np.full(len(plot_df), 0.55)\n",
    "    dy = np.full(len(plot_df), 0.6)\n",
    "    dz = plot_df[\"total_quantity\"].to_numpy(dtype=float)\n",
    "\n",
    "    revenue = plot_df[\"total_revenue\"].to_numpy(dtype=float)\n",
    "    vmin = float(revenue.min()) if len(revenue) else 0.0\n",
    "    vmax = float(revenue.max()) if len(revenue) else 1.0\n",
    "    if vmax <= vmin:\n",
    "        vmax = vmin + 1.0\n",
    "    norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    bar_colors = cm.YlOrRd(norm(revenue))\n",
    "\n",
    "    fig_3d = plt.figure(figsize=(16, 9))\n",
    "    ax = fig_3d.add_subplot(111, projection=\"3d\")\n",
    "    ax.bar3d(x, y, z, dx, dy, dz, color=bar_colors, shade=True, alpha=0.95)\n",
    "\n",
    "    ax.set_title(\"3D Orders: item x weekday x quantity\")\n",
    "    ax.set_xlabel(\"item (ref)\", labelpad=24)\n",
    "    ax.set_ylabel(\"weekday\", labelpad=20)\n",
    "    ax.set_zlabel(\"total_quantity\")\n",
    "    x_tick_positions = np.arange(len(item_order)) * x_spacing + 0.275\n",
    "    ax.set_xticks(x_tick_positions)\n",
    "    x_refs = [str(i + 1) for i in range(len(item_order))]\n",
    "    ax.set_xticklabels(x_refs, fontsize=9)\n",
    "    ax.tick_params(axis=\"x\", pad=4)\n",
    "    ax.set_xlim(-0.4, (len(item_order) - 1) * x_spacing + 1.0)\n",
    "    ax.set_yticks(np.arange(len(weekday_order)) + 0.3)\n",
    "    ax.set_yticklabels(weekday_order)\n",
    "    ax.set_box_aspect((1.8, 1.0, 0.8))\n",
    "    ax.view_init(elev=24, azim=228)\n",
    "\n",
    "    # Obracamy etykiety dni tygodnia zgodnie z kierunkiem osi Y na siatce.\n",
    "    x_min, x_max = ax.get_xlim3d()\n",
    "    z_min, _ = ax.get_zlim3d()\n",
    "    y0x, y0y, _ = proj3d.proj_transform(x_min, 0, z_min, ax.get_proj())\n",
    "    y1x, y1y, _ = proj3d.proj_transform(x_min, 1, z_min, ax.get_proj())\n",
    "    y_axis_angle = np.degrees(np.arctan2(y1y - y0y, y1x - y0x))\n",
    "    if y_axis_angle > 90:\n",
    "        y_axis_angle -= 180\n",
    "    if y_axis_angle < -90:\n",
    "        y_axis_angle += 180\n",
    "    fig_3d.canvas.draw()\n",
    "    for day_label in ax.get_yticklabels():\n",
    "        day_label.set_rotation(y_axis_angle + 50)\n",
    "        day_label.set_ha(\"right\")\n",
    "        day_label.set_va(\"center\")\n",
    "        day_label.set_rotation_mode(\"anchor\")\n",
    "\n",
    "    mapping_lines = [f\"{i + 1:>2} -> {item}\" for i, item in enumerate(item_order)]\n",
    "    mapping_text = \"Item reference:\\n\" + \"\\n\".join(mapping_lines)\n",
    "    fig_3d.text(\n",
    "        0.02,\n",
    "        0.5,\n",
    "        mapping_text,\n",
    "        ha=\"left\",\n",
    "        va=\"center\",\n",
    "        fontsize=9,\n",
    "        family=\"monospace\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"white\", alpha=0.85),\n",
    "    )\n",
    "\n",
    "    mappable = cm.ScalarMappable(norm=norm, cmap=cm.YlOrRd)\n",
    "    mappable.set_array([])\n",
    "    colorbar = fig_3d.colorbar(mappable, ax=ax, shrink=0.65, pad=0.08)\n",
    "    colorbar.set_label(\"total_revenue\")\n",
    "    fig_3d.subplots_adjust(left=0.20, right=0.92, bottom=0.08, top=0.92)\n",
    "    fig_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd9a0f2",
   "metadata": {},
   "source": [
    "## 7. Wykres: Heatmapa przychodu (dzien tygodnia x godzina)\n",
    "Pokazuje, kiedy w tygodniu i o jakiej godzinie wartosc sprzedazy jest najwyzsza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "weekday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "heatmap_spark_df = (\n",
    "    clean_df\n",
    "    .groupBy(\"weekday_num\", \"weekday_name\", \"hour_of_day\")\n",
    "    .agg(sum_agg(\"total_amount\").alias(\"total_revenue\"))\n",
    "    .orderBy(\"weekday_num\", \"hour_of_day\")\n",
    ")\n",
    "\n",
    "heatmap_df = heatmap_spark_df.toPandas()\n",
    "\n",
    "if heatmap_df.empty:\n",
    "    print(\"Brak danych do wykresu. Najpierw uruchom streaming i wyslij eventy.\")\n",
    "else:\n",
    "    heatmap_pivot = (\n",
    "        heatmap_df\n",
    "        .pivot_table(index=\"weekday_name\", columns=\"hour_of_day\", values=\"total_revenue\", aggfunc=\"sum\", fill_value=0.0)\n",
    "        .reindex(weekday_order)\n",
    "    )\n",
    "    heatmap_pivot = heatmap_pivot.reindex(columns=list(range(24)), fill_value=0.0)\n",
    "\n",
    "    fig_heatmap, ax = plt.subplots(figsize=(14, 5))\n",
    "    sns.heatmap(heatmap_pivot, cmap=\"YlOrRd\", ax=ax)\n",
    "    ax.set_title(\"Revenue heatmap: weekday x hour\")\n",
    "    ax.set_xlabel(\"hour_of_day\")\n",
    "    ax.set_ylabel(\"weekday\")\n",
    "    plt.tight_layout()\n",
    "    fig_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b720f0",
   "metadata": {},
   "source": [
    "## 8. Wykres Pareto: produkty wg przychodu\n",
    "Slupki pokazuja przychod produktu, linia pokazuje procent skumulowany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c623a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_spark_df = (\n",
    "    clean_df\n",
    "    .groupBy(\"item\")\n",
    "    .agg(sum_agg(\"total_amount\").alias(\"total_revenue\"))\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    ")\n",
    "\n",
    "pareto_df = pareto_spark_df.toPandas()\n",
    "\n",
    "if pareto_df.empty:\n",
    "    print(\"Brak danych do wykresu. Najpierw uruchom streaming i wyslij eventy.\")\n",
    "else:\n",
    "    pareto_df[\"cum_pct\"] = pareto_df[\"total_revenue\"].cumsum() / pareto_df[\"total_revenue\"].sum() * 100\n",
    "\n",
    "    fig_pareto, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    ax1.bar(pareto_df[\"item\"], pareto_df[\"total_revenue\"], color=\"#4c72b0\", alpha=0.9)\n",
    "    ax1.set_xlabel(\"item\")\n",
    "    ax1.set_ylabel(\"total_revenue\", color=\"#4c72b0\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"#4c72b0\")\n",
    "    ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pareto_df[\"item\"], pareto_df[\"cum_pct\"], color=\"#dd8452\", marker=\"o\", linewidth=2)\n",
    "    ax2.axhline(80, color=\"#55a868\", linestyle=\"--\", linewidth=1.5)\n",
    "    ax2.set_ylabel(\"cumulative %\", color=\"#dd8452\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"#dd8452\")\n",
    "\n",
    "    ax1.set_title(\"Pareto chart: revenue by item\")\n",
    "    plt.tight_layout()\n",
    "    fig_pareto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc91d97b",
   "metadata": {},
   "source": [
    "## 9. Boxplot: wartosc zamowienia wg kanalu i platnosci\n",
    "Porownuje rozklad `total_amount` pomiedzy kanalami sprzedazy i metodami platnosci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_df = clean_df.select(\"sales_channel\", \"payment_method\", \"total_amount\").toPandas()\n",
    "\n",
    "if boxplot_df.empty:\n",
    "    print(\"Brak danych do wykresu. Najpierw uruchom streaming i wyslij eventy.\")\n",
    "else:\n",
    "    fig_boxplot, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.boxplot(\n",
    "        data=boxplot_df,\n",
    "        x=\"sales_channel\",\n",
    "        y=\"total_amount\",\n",
    "        hue=\"payment_method\",\n",
    "        showfliers=False,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\"Order value by sales channel and payment method\")\n",
    "    ax.set_xlabel(\"sales_channel\")\n",
    "    ax.set_ylabel(\"total_amount\")\n",
    "    plt.tight_layout()\n",
    "    fig_boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79555c",
   "metadata": {},
   "source": [
    "## 10. Zapisz wykresy do plikow\n",
    "Zapisujemy wszystkie wygenerowane wykresy PNG w katalogu output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "charts_to_save = [\n",
    "    (\"fig_3d\", \"orders_3d_item_weekday.png\"),\n",
    "    (\"fig_heatmap\", \"orders_revenue_heatmap_weekday_hour.png\"),\n",
    "    (\"fig_pareto\", \"orders_pareto_revenue_item.png\"),\n",
    "    (\"fig_boxplot\", \"orders_boxplot_channel_payment.png\"),\n",
    "]\n",
    "\n",
    "saved_paths = []\n",
    "for fig_name, filename in charts_to_save:\n",
    "    figure_obj = globals().get(fig_name)\n",
    "    if figure_obj is None:\n",
    "        continue\n",
    "    chart_path = os.path.join(output_dir, filename)\n",
    "    figure_obj.savefig(chart_path, dpi=150, bbox_inches=\"tight\")\n",
    "    saved_paths.append(chart_path)\n",
    "\n",
    "if saved_paths:\n",
    "    saved_paths\n",
    "else:\n",
    "    print(\"Brak wykresow do zapisu. Uruchom komorki z wykresami.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
